{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bcdbb3",
   "metadata": {},
   "source": [
    "## Lab 3- Embeddings\n",
    "By Aryan Iyappan (2023115021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b8c96",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1a2cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['cat', 'fish', 'dog']\n",
      "One-hot encoded words: \n",
      "cat -> [1, 0, 0]\n",
      "dog -> [0, 0, 1]\n",
      "cat -> [1, 0, 0]\n",
      "fish -> [0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "words = [\"cat\", \"dog\", \"cat\", \"fish\"]\n",
    "\n",
    "vocab = list(set(words))\n",
    "\n",
    "one_hot = []\n",
    "\n",
    "for word in words:\n",
    "    vector = [1 if v == word else 0 for v in vocab]\n",
    "    one_hot.append(vector)\n",
    "\n",
    "print(\"Vocabulary: \", vocab)\n",
    "print(\"One-hot encoded words: \")\n",
    "for w, v in zip(words, one_hot):\n",
    "    print(w, \"->\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643c9e8",
   "metadata": {},
   "source": [
    "### One-hot encoding using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c083d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cat    dog   fish\n",
      "0   True  False  False\n",
      "1  False   True  False\n",
      "2  False  False   True\n",
      "3   True  False  False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.Series([\"cat\", \"dog\", \"fish\", \"cat\"])\n",
    "\n",
    "print(pd.get_dummies(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b8e32",
   "metadata": {},
   "source": [
    "### One-hot encoding using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca62505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories:  [array(['cat', 'dog', 'fish'], dtype='<U4')]\n",
      "One hot encoded output: \n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[\"cat\"], [\"dog\"], [\"fish\"], [\"cat\"]])\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "encoded = encoder.fit_transform(data)\n",
    "\n",
    "print(\"Categories: \", encoder.categories_)\n",
    "\n",
    "print(\"One hot encoded output: \")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb2147",
   "metadata": {},
   "source": [
    "### Word2Vec implementation using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfddc67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for intelligence: \n",
      "[-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
      "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
      " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
      " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
      "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
      "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
      "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
      " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
      "  0.00302193  0.00358008]\n",
      "similar words to learning:  [('machine', 0.16704076528549194), ('deep', 0.13204392790794373), ('intelligence', 0.1267007291316986), ('part', 0.0998455286026001), ('is', 0.042373016476631165), ('powerful', 0.04067763686180115), ('neural', 0.012442179024219513), ('a', -0.01259106956422329), ('artificial', -0.01447527389973402), ('of', -0.0560765340924263)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    \"artificial intelligence is powerful\".split(\" \"),\n",
    "    \"machine learning is a part of artificial intelligence\".split(\" \"),\n",
    "    \"deep learning uses neural networks\".split(\" \")\n",
    "]\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    ")\n",
    "\n",
    "vector = model.wv[\"intelligence\"]\n",
    "\n",
    "print(\"Vector for intelligence: \")\n",
    "print(vector)\n",
    "\n",
    "similar_words = model.wv.most_similar(\"learning\")\n",
    "print(\"similar words to learning: \", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0114a",
   "metadata": {},
   "source": [
    "### GloVe implementation using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4fba397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "Vector for intelligence: \n",
      "[-0.31101   -0.43291    0.77734   -0.31115    0.052934  -0.8502\n",
      " -0.35372   -0.70531    0.084464   0.88768    0.83527   -0.41641\n",
      "  0.36703    0.60834    0.0085214  0.94293    0.5314    -0.75322\n",
      " -0.86764    0.34833   -0.29865   -0.43442    0.3514    -1.1228\n",
      " -1.2564    -0.094171   0.29402    0.31994    0.086692   0.31915\n",
      "  0.56067    0.032952  -0.94379   -0.58112    0.11274    0.006062\n",
      " -0.79353    0.70368    0.59687    0.60501   -0.22855   -0.26469\n",
      "  0.045172   0.58118    0.26756   -0.47237    0.29358   -0.28342\n",
      " -0.22823   -0.59532    1.0845     0.21541    0.5789     1.5825\n",
      "  0.15322   -1.3246     0.42594   -0.24834    1.3285     0.48737\n",
      "  0.17115    0.73042    0.51749   -0.50172    0.23246   -0.33179\n",
      " -0.31772    0.34714    0.95887    1.5972     0.76459   -0.1559\n",
      " -0.13554   -0.97654   -0.29545    0.097254  -0.17109    0.17695\n",
      " -1.1941     0.41086    1.0578     0.55551    0.034317  -0.18596\n",
      " -1.7366     0.22696    1.0213     0.80212   -0.017432  -0.45574\n",
      " -0.11358    0.032074  -0.37083    0.22161   -0.0030162  0.23286\n",
      "  0.16984   -1.0727    -0.18416    0.45819  ]\n",
      "teaching: 0.7720069289207458\n",
      "knowledge: 0.76064133644104\n",
      "experience: 0.7378624081611633\n",
      "skills: 0.734375\n",
      "learn: 0.7340658903121948\n",
      "understanding: 0.699902355670929\n",
      "education: 0.6989468336105347\n",
      "educational: 0.6864885687828064\n",
      "lessons: 0.681064784526825\n",
      "studying: 0.6762653589248657\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "vector = glove_model[\"intelligence\"]\n",
    "\n",
    "print(\"Vector for intelligence: \")\n",
    "print(vector)\n",
    "\n",
    "similar_words = glove_model.most_similar(\"learning\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6dadc",
   "metadata": {},
   "source": [
    "### FastText embedding implmentation (using gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb67e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for intelligence: \n",
      "[ 1.2442525e-03  6.1205524e-04 -7.8747980e-04  1.6670383e-04\n",
      " -2.2123381e-03 -1.2123872e-03 -1.2194589e-03 -3.8544473e-03\n",
      "  2.5588896e-03  1.5041081e-03  5.0475444e-03  1.2285195e-03\n",
      "  8.7324705e-04 -1.1611626e-04 -7.7292963e-04 -2.4581761e-03\n",
      " -1.2628294e-03 -1.8496130e-03  4.2046177e-05  2.7340525e-03\n",
      "  1.2192813e-03  1.2515471e-03  2.6138644e-03 -5.6116347e-04\n",
      "  1.2510451e-03  8.1061118e-04  2.9067504e-03  3.9396860e-04\n",
      " -5.7816796e-04 -2.8444370e-03  2.1801500e-03 -2.8956099e-04\n",
      " -2.9883529e-03 -3.0058046e-04 -7.7108195e-04  6.8529876e-04\n",
      " -1.3268661e-03  1.2676803e-03 -8.3737250e-04  9.6935441e-04\n",
      "  1.9220284e-03 -2.5679730e-03 -1.1573596e-03  1.0904556e-03\n",
      "  1.4968399e-03  1.2666686e-03  1.1835489e-03  7.3763513e-04\n",
      " -2.3163224e-04 -1.6384197e-03]\n",
      "processing: 0.2987723648548126\n",
      "extension: 0.23967039585113525\n",
      "embeddings: 0.2296343594789505\n",
      "an: 0.2120790332555771\n",
      "capture: 0.2046079933643341\n",
      "language: 0.18014957010746002\n",
      "fascinating: 0.1578020453453064\n",
      "meaning: 0.1501166671514511\n",
      "word: 0.11600217968225479\n",
      "natural: 0.10092004388570786\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [\n",
    "    \"natural language processing is a fascinating field\".split(\" \"),\n",
    "    \"word embeddings capture semantic meaning\".split(\" \"),\n",
    "    \"fasttext is an extension of word2vec\".split(\" \")\n",
    "]\n",
    "\n",
    "model = FastText(\n",
    "    sentences,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    ")\n",
    "\n",
    "vector = model.wv[\"intelligence\"]\n",
    "print(\"Vector for intelligence: \")\n",
    "print(vector)\n",
    "\n",
    "similar_words = model.wv.most_similar(\"learning\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add17e1f",
   "metadata": {},
   "source": [
    "## Try it yourself problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4208000",
   "metadata": {},
   "source": [
    "Problem 1: Word2Vec Semantic Similarity \n",
    "\n",
    "Create a small text corpus related to artificial intelligence and train a Word2Vec model using the Skip-Gram approach. Extract the vector representation of a given word and compute the top five most similar words. Observe whether semantically related words appear closer in the embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c234c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for artificial: \n",
      "[-0.01631689  0.00898098 -0.00828151  0.00164911  0.01698853 -0.00893017\n",
      "  0.00903413 -0.01358104 -0.00710486  0.01878957 -0.00315465  0.00063616\n",
      " -0.00827822 -0.01536349 -0.00301752  0.00494668 -0.00178007  0.01106599\n",
      " -0.00549287  0.00451755  0.01090922  0.01669611 -0.0029087  -0.01840873\n",
      "  0.00874684  0.00113891  0.01488712 -0.00161985 -0.00528167 -0.01750022\n",
      " -0.00170869  0.00564818  0.01079729  0.01410526 -0.01140808  0.00371395\n",
      "  0.01218884 -0.00960119 -0.00622252  0.01359342  0.00326335  0.00037754\n",
      "  0.00694279  0.00043278  0.01923393  0.01012419 -0.0178245  -0.01409015\n",
      "  0.00179851  0.01278906]\n",
      "neural: 0.07401131093502045\n",
      "intelligence: 0.04244506359100342\n",
      "networks: 0.018417924642562866\n",
      "subset: 0.011702703312039375\n",
      "of: 0.011397400870919228\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    \"artificial intelligence is the electricity of the 21st century\".split(\" \"),\n",
    "    \"machine learning is a subset of artificial intelligence\".split(\" \"),\n",
    "    \"artificial intelligence has evolved from simple algorithms to complex neural networks\".split(\" \")\n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "vector = model.wv[\"artificial\"]\n",
    "print(\"Vector for artificial: \")\n",
    "print(vector)\n",
    "\n",
    "similar_words = model.wv.most_similar(\"artificial\")\n",
    "for word, score in similar_words[5:]:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794488e",
   "metadata": {},
   "source": [
    "Problem 2: Using Pretrained GloVe Embeddings \n",
    "\n",
    "Load pretrained GloVe embeddings and retrieve vectors for selected words such as “learning,” “education,” and “knowledge.” Compute cosine similarity between pairs of words and analyze which pairs show stronger semantic similarity based on the similarity score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69f982e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for learning: \n",
      "[ 0.64812    0.69878   -0.39947    0.77634   -0.13132    0.2024\n",
      " -0.33399   -0.0066588  0.061684   0.1885    -0.10559   -0.31316\n",
      " -0.082495  -0.080517   0.3858    -0.10302    0.049431   0.17216\n",
      " -0.59079    0.77068   -1.2768    -0.25187    0.2195    -0.20176\n",
      " -0.30581   -0.18518    0.010889  -0.07529   -0.34732    0.61998\n",
      " -0.99703    1.0516    -0.42071   -0.39635    0.32607   -0.40061\n",
      " -0.46462    0.69904    0.29567   -0.35309   -0.59074    0.28999\n",
      " -0.25732   -0.1317    -0.69798    0.49818    0.41503    0.1487\n",
      "  0.083347  -0.43543   -0.093969  -0.3543     0.014998   0.63593\n",
      "  0.54564   -1.8439     0.78842   -0.19836    1.5707     0.25988\n",
      "  0.20875    0.7521    -0.085488  -0.70717    0.094104   0.44485\n",
      "  0.087818  -0.34779    0.57148    0.18662   -0.29435    0.42928\n",
      "  0.28392   -0.61614   -0.34108    0.58192   -0.16388   -0.0081997\n",
      " -0.27162   -0.27112   -0.21471    0.37376   -0.5352    -0.060945\n",
      " -1.6317     0.85144    0.056035  -0.53861   -0.58383   -0.19612\n",
      " -0.33941   -0.3141     0.22999    0.32688    0.043012  -0.037482\n",
      " -0.092067  -1.0734     0.8924     0.41776  ]\n",
      "Vector for education: \n",
      "[ 0.29576    0.76822   -0.64301    0.3542     0.4718    -0.1886\n",
      " -0.4511     0.4861     0.037184   0.89778   -0.4344    -0.6782\n",
      " -0.52404    0.67126    0.051964  -0.84691    0.45369    0.038235\n",
      " -1.1173     0.61603   -1.1429     0.040068   0.68673   -0.099044\n",
      " -0.82311   -0.46081    0.21694   -0.91177   -1.0395     0.1033\n",
      " -0.74415    0.96167    0.063783   0.038383  -0.63326    0.072576\n",
      " -0.96552    0.80286   -0.97659    0.53129   -0.60987   -0.13728\n",
      " -0.32801   -0.09886   -0.095179  -0.34709    0.32289    0.29248\n",
      " -0.20091   -0.48917   -0.071877  -0.29989   -0.56343    0.78851\n",
      "  0.15249   -1.8269     0.91582   -0.39346    2.0874     0.020593\n",
      "  0.28569   -0.41762   -0.0403    -0.5069     0.57702    0.26037\n",
      " -0.0097665  0.18719    1.4409     0.48926    0.46      -0.097184\n",
      "  0.061867  -0.37088   -0.25459    0.43804   -0.61944    0.40353\n",
      " -0.86386   -1.201     -0.42141    0.50534    0.091196  -0.65938\n",
      " -1.731      0.50761   -0.38395   -0.40393   -0.08251   -0.58137\n",
      "  0.37132   -0.76512   -0.070605   0.64349   -0.60163    0.14938\n",
      "  0.28977   -0.62661    0.5522    -0.068743 ]\n",
      "Vector for knowledge: \n",
      "[ 0.37873   0.29093   0.33448   0.81958   0.02157  -0.33902  -0.18786\n",
      " -0.81997   0.21917   0.32912   0.056336 -0.021378  0.4227   -0.033901\n",
      "  0.10828  -0.26949   0.43562   0.62068  -0.82773   0.60077  -1.0572\n",
      " -0.26677  -0.071193 -0.81948  -0.50997  -0.22701   0.62731  -0.035517\n",
      " -0.33592   0.05169  -0.44974   0.54877  -0.88611  -0.56539   0.049035\n",
      " -0.11677  -0.46125   0.65521  -0.20819  -0.01756  -0.69696   0.029489\n",
      "  0.14687  -0.21396  -0.56799   0.056679  0.19227   0.069002  0.1914\n",
      " -0.52445   0.34553   0.63177   0.28517   0.84309   0.30828  -1.2652\n",
      "  0.38705  -0.57428   1.6067    0.042609  0.20763   0.64654  -0.21176\n",
      " -0.39534   0.74169  -0.33656   0.32451  -0.6909    0.72468   0.23863\n",
      " -0.083783  0.28153   0.62182  -0.74839  -0.093944  0.13944  -0.22561\n",
      " -0.53502  -1.0727    0.073157  0.45179   0.90429  -0.23078  -0.35609\n",
      " -1.6446    0.42241   0.019176 -0.038683 -0.087027  0.16006  -0.023005\n",
      " -0.31278   0.27434   0.3404    0.48221  -0.14243  -0.38773  -1.346\n",
      "  0.17863   0.95068 ]\n",
      "Computing cosine similarities:\n",
      "Cosine similarity between learning and education: 0.6989467740058899\n",
      "Cosine similarity between learning and knowledge: 0.76064133644104\n",
      "Cosine similarity between education and knowledge: 0.5934725999832153\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "selected_words = [\"learning\", \"education\", \"knowledge\"]\n",
    "\n",
    "for word in selected_words:\n",
    "    vector = glove_model[word]\n",
    "    print(f\"Vector for {word}: \")\n",
    "    print(vector)\n",
    "\n",
    "print(\"Computing cosine similarities:\")\n",
    "for i, word1 in enumerate(selected_words):\n",
    "    for word2 in selected_words[i + 1:]:\n",
    "        similarity = glove_model.similarity(word1, word2)\n",
    "        print(f\"Cosine similarity between {word1} and {word2}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b1666",
   "metadata": {},
   "source": [
    "Problem 3: FastText and Out-of-Vocabulary Words \n",
    "\n",
    "Train a FastText model on a small corpus and generate embeddings for both seen and unseen words. Compare how FastText produces meaningful vectors for unseen words due to its character n-gram representation, unlike Word2Vec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e98c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for seen word artificial: \n",
      "[-2.1826662e-03  6.0296047e-04 -1.4207925e-03 -8.9026283e-04\n",
      " -2.4187756e-03  2.9780034e-03  1.7206641e-03  2.7639512e-03\n",
      " -1.9971149e-03  4.0762232e-05  8.7178743e-04 -2.3102832e-03\n",
      "  3.2494627e-03 -5.3905376e-04  2.5887699e-03  2.1144887e-03\n",
      "  6.7115214e-04  3.0500221e-03  3.8534440e-03  1.6691759e-03\n",
      "  5.5586832e-04 -1.1562546e-03  6.8903202e-04  1.9583234e-03\n",
      " -1.5651557e-03 -7.0948462e-04  1.9376080e-04  2.8539493e-04\n",
      " -3.3578160e-04 -2.6936035e-03  1.3029205e-03 -3.2696521e-03\n",
      "  4.5152735e-03 -1.5075026e-04 -2.1355576e-03 -1.2526751e-03\n",
      "  1.1347395e-03  1.1043815e-03 -1.3895440e-03 -1.0036826e-04\n",
      "  3.4054377e-04 -4.3776690e-04  1.8515515e-03 -3.6265829e-03\n",
      "  3.4345223e-03 -2.0397338e-04 -1.0703121e-03  1.1549455e-03\n",
      " -3.8344195e-04  9.8625629e-04]\n",
      "Vector for seen word intelligence: \n",
      "[ 0.0008144   0.00076832 -0.00052778  0.00042989 -0.00181359 -0.00147106\n",
      " -0.00113968 -0.00348354  0.00236729  0.00118199  0.00491108  0.0008107\n",
      "  0.00059247  0.00021703 -0.00059902 -0.00206493 -0.00091717 -0.00145633\n",
      " -0.00013517  0.00264434  0.00130015  0.00101225  0.00294325 -0.00100663\n",
      "  0.00153659  0.00092731  0.00260972  0.00058937 -0.00064563 -0.00246613\n",
      "  0.00259293 -0.00048573 -0.00294673 -0.0005585  -0.00057404  0.00079897\n",
      " -0.00097549  0.00152197 -0.00037425  0.00137813  0.00224468 -0.00283335\n",
      " -0.0015563   0.00104855  0.00131785  0.00160439  0.00143224  0.00064859\n",
      " -0.00015597 -0.00151706]\n",
      "Vector for seen word learning: \n",
      "[-1.05286681e-03 -1.08409049e-05  1.21094490e-04 -1.31922041e-03\n",
      " -2.18495075e-03  1.00735226e-03  2.70417822e-03 -1.13853115e-04\n",
      " -3.89148272e-03  1.27833011e-03  8.04895128e-04 -1.30332168e-03\n",
      " -3.16125952e-04  2.29635881e-03  7.91175524e-04 -1.40045898e-03\n",
      "  1.81247061e-03 -7.56660767e-04 -9.75602365e-04  2.86916108e-03\n",
      "  1.88996887e-03 -6.14976045e-04 -2.78176228e-03  2.43432703e-03\n",
      "  2.23854859e-03 -2.58058170e-03 -2.55914358e-03  3.04997223e-03\n",
      " -1.73147523e-03  4.60855523e-03  2.61381618e-03  2.76661851e-03\n",
      " -1.75107515e-03  5.43446979e-04 -1.02689362e-03 -1.36143796e-03\n",
      "  6.90879242e-04  5.28758566e-04  4.00449755e-03  4.46086191e-03\n",
      "  2.30123871e-03  5.95618738e-04  2.77006230e-03 -6.24841559e-05\n",
      " -1.52975030e-03 -1.03771826e-03 -4.34575835e-03 -3.39414296e-03\n",
      " -2.13169167e-03  1.60493923e-03]\n",
      "Vector for unseen word automation: \n",
      "[ 1.3569385e-03 -2.7879509e-03  1.9570920e-03  1.8054819e-04\n",
      " -1.4279447e-03 -9.2771801e-04  5.7319802e-04 -2.8440226e-03\n",
      " -8.8683312e-04  2.0163697e-03 -1.5524941e-03  1.3554433e-03\n",
      " -1.7240945e-03  2.4476529e-03 -8.2642946e-05  2.7297335e-03\n",
      " -3.6950931e-03  1.4047219e-03 -1.2119859e-03  1.4929721e-03\n",
      " -1.9195823e-03  1.5742444e-03 -6.8690110e-04 -3.6159158e-03\n",
      "  2.8763586e-03  1.1977248e-03 -2.2659395e-03 -3.9178063e-04\n",
      "  1.0054506e-03 -1.3474933e-03  1.7761486e-03  1.2831105e-03\n",
      " -4.2290150e-04 -1.7497322e-04  2.2821254e-03  1.0766730e-03\n",
      " -1.1582710e-03 -1.6064808e-04 -1.4781263e-03 -4.0800491e-04\n",
      "  2.9161149e-03  3.2887075e-04  1.3949247e-03 -6.1488949e-04\n",
      " -1.2428754e-03  2.0528995e-03 -1.0926848e-03 -5.6113116e-05\n",
      " -1.1239484e-03 -5.4760056e-04]\n",
      "Vector for unseen word robotics: \n",
      "[ 3.8774160e-03 -3.4782593e-03  1.5943999e-03  2.0833319e-04\n",
      " -1.6199872e-03 -2.2620497e-04 -1.1893415e-03 -6.7674916e-04\n",
      "  1.4396107e-03  1.5082471e-03 -2.1150161e-03 -1.4287385e-03\n",
      "  6.0346018e-04  1.7633376e-03 -1.6511793e-04 -1.9417789e-03\n",
      "  2.1342894e-03 -1.9526712e-03 -1.1768372e-03  4.1033034e-04\n",
      " -7.6792069e-04  9.0129790e-04  1.3423754e-03 -7.8982244e-05\n",
      " -6.5456057e-04 -2.4927980e-03 -1.5609680e-03 -1.1788810e-03\n",
      " -4.8434134e-03 -1.5379037e-04 -9.0091978e-04 -5.7965391e-03\n",
      " -1.2290573e-03 -3.9954553e-03 -9.5519744e-04  1.9540498e-03\n",
      " -2.5690892e-03  4.0853391e-03 -1.5457519e-03  2.3401440e-03\n",
      " -1.3759176e-03 -3.0025787e-04  1.8748117e-03 -3.8863804e-05\n",
      " -8.0855179e-04 -2.3168765e-03  2.0454281e-03  1.8012740e-03\n",
      "  1.1493039e-03  2.9864551e-03]\n",
      "Word 'automation' not found in Word2Vec vocabulary.\n",
      "Word 'robotics' not found in Word2Vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [\n",
    "    \"artificial intelligence is powerful\".split(\" \"),\n",
    "    \"machine learning is a part of artificial intelligence\".split(\" \"),\n",
    "    \"deep learning uses neural networks\".split(\" \")\n",
    "]\n",
    "\n",
    "model = FastText(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "# seen words\n",
    "seen_words = [\"artificial\", \"intelligence\", \"learning\"]\n",
    "for word in seen_words:\n",
    "    vector = model.wv[word]\n",
    "    print(f\"Vector for seen word {word}: \")\n",
    "    print(vector)\n",
    "\n",
    "# unseen words\n",
    "unseen_words = [\"automation\", \"robotics\"]\n",
    "for word in unseen_words:\n",
    "    vector = model.wv[word]\n",
    "    print(f\"Vector for unseen word {word}: \")\n",
    "    print(vector)\n",
    "\n",
    "# unseen words by word2vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "for word in unseen_words:\n",
    "    try:\n",
    "        vector = word2vec_model.wv[word]\n",
    "        print(f\"Vector for unseen word {word} in Word2Vec: \")\n",
    "        print(vector)\n",
    "    except KeyError:\n",
    "        print(f\"Word '{word}' not found in Word2Vec vocabulary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
